{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_validation_early_exit_dnn_mbdi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiet4T_SCIEA",
        "outputId": "b0931b4a-7e8b-4593-fea7-de2044845909"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.utils import save_image\n",
        "import os, cv2, sys, time, math, os\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n",
        "from torch.utils.data import random_split\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms\n",
        "!pip install pthflops\n",
        "from pthflops import count_ops\n",
        "from pthflops import count_ops\n",
        "from torch import Tensor\n",
        "from typing import Callable, Any, Optional, List"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pthflops\n",
            "  Downloading pthflops-0.4.1.tar.gz (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pthflops) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pthflops) (3.7.4.3)\n",
            "Building wheels for collected packages: pthflops\n",
            "  Building wheel for pthflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pthflops: filename=pthflops-0.4.1-py3-none-any.whl size=10063 sha256=96bad1f9a45dbeb40a6f36c2f65e51d4a01a104d034e6fbfe4fdad9adadcabb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/41/05/475bdaebaaf3a44f25367a8dc0ac9d4b8edbb7f5fa19724c70\n",
            "Successfully built pthflops\n",
            "Installing collected packages: pthflops\n",
            "Successfully installed pthflops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGm1epsC6xA"
      },
      "source": [
        "class LoadDataset():\n",
        "  def __init__(self, input_dim, batch_size_train, batch_size_test, save_idx, model_id, seed=42):\n",
        "    self.input_dim = input_dim\n",
        "    self.batch_size_train = batch_size_train\n",
        "    self.batch_size_test = batch_size_test\n",
        "    self.seed = seed\n",
        "    self.save_idx = save_idx\n",
        "    self.model_id = model_id\n",
        "\n",
        "    #To normalize the input images data.\n",
        "    mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n",
        "    std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n",
        "\n",
        "    # Note that we apply data augmentation in the training dataset.\n",
        "    self.transformations_train = transforms.Compose([transforms.Resize((input_dim, input_dim)),\n",
        "                                                     transforms.RandomChoice([\n",
        "                                                                              transforms.ColorJitter(brightness=(0.80, 1.20)),\n",
        "                                                                              transforms.RandomGrayscale(p = 0.25)]),\n",
        "                                                     transforms.RandomHorizontalFlip(p = 0.25),\n",
        "                                                     transforms.RandomRotation(25),\n",
        "                                                     transforms.ToTensor(), \n",
        "                                                     transforms.Normalize(mean = mean, std = std),\n",
        "                                                     ])\n",
        "\n",
        "    # Note that we do not apply data augmentation in the test dataset.\n",
        "    self.transformations_test = transforms.Compose([\n",
        "                                                     transforms.Resize(input_dim), \n",
        "                                                     transforms.CenterCrop(input_dim), \n",
        "                                                     transforms.ToTensor(), \n",
        "                                                     transforms.Normalize(mean = mean, std = std),\n",
        "                                                     ])\n",
        "\n",
        "  def cifar_10(self, root_path, split_ratio):\n",
        "    # This method loads Cifar-10 dataset. \n",
        "    \n",
        "    # saves the seed\n",
        "    torch.manual_seed(self.seed)\n",
        "\n",
        "    # This downloads the training and test CIFAR-10 datasets and also applies transformation  in the data.\n",
        "    train_set = datasets.CIFAR10(root=root_path, train=True, download=True, transform=self.transformations_train)\n",
        "    test_set = datasets.CIFAR10(root=root_path, train=False, download=True, transform=self.transformations_test)\n",
        "\n",
        "    classes_list = train_set.classes\n",
        "\n",
        "    # This line defines the size of validation dataset.\n",
        "    val_size = int(split_ratio*len(train_set))\n",
        "\n",
        "    # This line defines the size of training dataset.\n",
        "    train_size = int(len(train_set) - val_size)\n",
        "\n",
        "    #This line splits the training dataset into train and validation, according split ratio provided as input.\n",
        "    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "    #This block creates data loaders for training, validation and test datasets.\n",
        "    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "  def cifar_100(self, root_path, split_ratio):\n",
        "    # This method loads Cifar-100 dataset\n",
        "    root = \"cifar_100\"\n",
        "    torch.manual_seed(self.seed)\n",
        "\n",
        "    # This downloads the training and test Cifar-100 datasets and also applies transformation  in the data.\n",
        "    train_set = datasets.CIFAR100(root=root_path, train=True, download=True, transform=self.transformations_train)\n",
        "    test_set = datasets.CIFAR100(root=root_path, train=False, download=True, transform=self.transformations_train)\n",
        "\n",
        "    classes_list = train_set.classes\n",
        "\n",
        "    # This line defines the size of validation dataset.\n",
        "    val_size = int(split_ratio*len(train_set))\n",
        "\n",
        "    # This line defines the size of training dataset.\n",
        "    train_size = int(len(train_set) - val_size)\n",
        "\n",
        "    #This line splits the training dataset into train and validation, according split ratio provided as input.\n",
        "    train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "    #This block creates data loaders for training, validation and test datasets.\n",
        "    train_loader = DataLoader(train_dataset, self.batch_size_train, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, self.batch_size_test, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, self.batch_size_test, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "  \n",
        "  def get_indices(self, dataset, split_ratio):\n",
        "    nr_samples = len(dataset)\n",
        "    indices = list(range(nr_samples))\n",
        "    \n",
        "    train_size = nr_samples - int(np.floor(split_ratio * nr_samples))\n",
        "\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
        "\n",
        "    return train_idx, test_idx\n",
        "\n",
        "  def caltech_256(self, root_path, split_ratio, savePath_idx):\n",
        "    # This method loads the Caltech-256 dataset.\n",
        "\n",
        "    torch.manual_seed(self.seed)\n",
        "    np.random.seed(seed=None)\n",
        "\n",
        "    # This block receives the dataset path and applies the transformation data. \n",
        "    train_set = datasets.ImageFolder(root_path, transform=self.transformations_train)\n",
        "\n",
        "    val_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n",
        "    test_set = datasets.ImageFolder(root_path, transform=self.transformations_test)\n",
        "\n",
        "    if (os.path.exists(os.path.join(savePath_idx, \"training_idx_caltech256_id_%s.npy\"%(self.model_id)))):\n",
        "      \n",
        "      train_idx = np.load(os.path.join(savePath_idx, \"training_idx_caltech256_id_%s.npy\"%(self.model_id)))\n",
        "      val_idx = np.load(os.path.join(savePath_idx, \"validation_idx_caltech256_id_%s.npy\"%(self.model_id)))\n",
        "      test_idx = np.load(os.path.join(savePath_idx, \"test_idx_caltech256_id_%s.npy\"%(self.model_id)))\n",
        "\n",
        "    else:\n",
        "\n",
        "      # This line get the indices of the samples which belong to the training dataset and test dataset. \n",
        "      train_idx, test_idx = self.get_indices(train_set, split_ratio)\n",
        "\n",
        "      # This line mounts the training and test dataset, selecting the samples according indices. \n",
        "      train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n",
        "      test_data = torch.utils.data.Subset(test_set, indices=test_idx)\n",
        "\n",
        "      # This line gets the indices to split the train dataset into training dataset and validation dataset.\n",
        "      train_idx, val_idx = self.get_indices(train_data, split_ratio)\n",
        "\n",
        "      np.save(os.path.join(savePath_idx, \"traning_idx_caltech256_id_%s.npy\"%(self.model_id)), train_idx)\n",
        "      np.save(os.path.join(savePath_idx, \"validation_idx_caltech256_id_%s.npy\"%(self.model_id)), val_idx)\n",
        "      np.save(os.path.join(savePath_idx, \"test_idx_caltech256_id_%s.npy\"%(self.model_id)), test_idx)\n",
        "\n",
        "    train_data = torch.utils.data.Subset(train_set, indices=train_idx)\n",
        "    val_data = torch.utils.data.Subset(val_set, indices=val_idx)\n",
        "    test_data = torch.utils.data.Subset(test_set, indices=test_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size_train, shuffle=True, num_workers=4)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=self.batch_size_test, num_workers=4)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=self.batch_size_test, num_workers=4)\n",
        "\n",
        "    return train_loader, val_loader, test_loader \n",
        "\n",
        "  def getDataset(self, root_path, dataset_name, split_ratio, savePath_idx):\n",
        "    self.dataset_name = dataset_name\n",
        "    def func_not_found():\n",
        "      print(\"No dataset %s is found\"%(self.dataset_name))\n",
        "\n",
        "    func_name = getattr(self, self.dataset_name, func_not_found)\n",
        "    train_loader, val_loader, test_loader = func_name(root_path, split_ratio, savePath_idx)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "#input_dim = 224\n",
        "#batch_size_train = 128\n",
        "#batch_size_test = 1\n",
        "#split_ratio = 0.1\n",
        "#dataset_root_path = \"./drive/MyDrive/undistorted_datasets/Caltech256/256_ObjectCategories/\"\n",
        "#dataset = LoadDataset(input_dim, batch_size_train, batch_size_test)\n",
        "#dataset.getDataset(\"cifar10\", \"cifar_10\", split_ratio)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXSN1FCKEY2O"
      },
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "  \"\"\"3x3 convolution with padding\"\"\"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  \"\"\"Basic Block defition.\n",
        "  Basic 3X3 convolution blocks for use on ResNets with layers <= 34.\n",
        "  Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "  \"\"\"\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "class ConvBNActivation(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_planes: int,\n",
        "        out_planes: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        dilation: int = 1,\n",
        "    ) -> None:\n",
        "        padding = (kernel_size - 1) // 2 * dilation\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if activation_layer is None:\n",
        "            activation_layer = nn.ReLU6\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
        "                      bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            activation_layer(inplace=True)\n",
        "        )\n",
        "        self.out_channels = out_planes\n",
        "\n",
        "\n",
        "# necessary for backwards compatibility\n",
        "ConvBNReLU = ConvBNActivation\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inp: int,\n",
        "        oup: int,\n",
        "        stride: int,\n",
        "        expand_ratio: int,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            norm_layer(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.out_channels = oup\n",
        "        self._is_cn = stride > 1\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "class EarlyExitBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  This EarlyExitBlock allows the model to terminate early when it is confident for classification.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape, n_classes, exit_type, device):\n",
        "    super(EarlyExitBlock, self).__init__()\n",
        "    self.input_shape = input_shape\n",
        "\n",
        "    _, channel, width, height = input_shape\n",
        "    self.expansion = width * height if exit_type == 'plain' else 1\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    if (exit_type == 'bnpool'):\n",
        "      self.layers.append(nn.BatchNorm2d(channel))\n",
        "\n",
        "    if (exit_type != 'plain'):\n",
        "      self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
        "    \n",
        "    #This line defines the data shape that fully-connected layer receives.\n",
        "    current_channel, current_width, current_height = self.get_current_data_shape()\n",
        "\n",
        "    self.layers = self.layers.to(device)\n",
        "\n",
        "    #This line builds the fully-connected layer\n",
        "    self.classifier = nn.Sequential(nn.Linear(current_channel*current_width*current_height, n_classes)).to(device)\n",
        "\n",
        "    self.softmax_layer = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def get_current_data_shape(self):\n",
        "    _, channel, width, height = self.input_shape\n",
        "    temp_layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    input_tensor = torch.rand(1, channel, width, height)\n",
        "    _, output_channel, output_width, output_height = temp_layers(input_tensor).shape\n",
        "    return output_channel, output_width, output_height\n",
        "        \n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    output = self.classifier(x)\n",
        "    #confidence = self.softmax_layer()\n",
        "    return output\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "  \"\"\"1x1 convolution\"\"\"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Early_Exit_DNN(nn.Module):\n",
        "  def __init__(self, model_name: str, n_classes: int, \n",
        "               pretrained: bool, n_branches: int, input_shape:tuple, \n",
        "               exit_type: str, device, distribution=\"linear\"):\n",
        "    super(Early_Exit_DNN, self).__init__()\n",
        "\n",
        "    \"\"\"\n",
        "    This classes builds an early-exit DNNs architectures\n",
        "    Args:\n",
        "\n",
        "    model_name: model name \n",
        "    n_classes: number of classes in a classification problem, according to the dataset\n",
        "    pretrained: \n",
        "    n_branches: number of branches (early exits) inserted into middle layers\n",
        "    input_shape: shape of the input image\n",
        "    exit_type: type of the exits\n",
        "    distribution: distribution method of the early exit blocks.\n",
        "    device: indicates if the model will processed in the cpu or in gpu\n",
        "    \n",
        "    Note: the term \"backbone model\" refers to a regular DNN model, considering no early exits.\n",
        "\n",
        "    \"\"\"\n",
        "    self.model_name = model_name\n",
        "    self.n_classes = n_classes\n",
        "    self.pretrained = pretrained\n",
        "    self.n_branches = n_branches\n",
        "    self.input_shape = input_shape\n",
        "    self.exit_type = exit_type\n",
        "    self.distribution = distribution\n",
        "    self.device = device\n",
        "    self.channel, self.width, self.height = input_shape\n",
        "\n",
        "\n",
        "    build_early_exit_dnn = self.select_dnn_architecture_model()\n",
        "\n",
        "    build_early_exit_dnn()\n",
        "\n",
        "  def select_dnn_architecture_model(self):\n",
        "    \"\"\"\n",
        "    This method selects the backbone to insert the early exits.\n",
        "    \"\"\"\n",
        "\n",
        "    architecture_dnn_model_dict = {\"alexnet\": self.early_exit_alexnet,\n",
        "                                   \"mobilenet\": self.early_exit_mobilenet,\n",
        "                                   \"resnet18\": self.early_exit_resnet18,\n",
        "                                   \"resnet34\": self.early_exit_resnet34}\n",
        "\n",
        "    return architecture_dnn_model_dict.get(self.model_name, self.invalid_model)\n",
        "\n",
        "  def select_distribution_method(self):\n",
        "    \"\"\"\n",
        "    This method selects the distribution method to insert early exits into the middle layers.\n",
        "    \"\"\"\n",
        "    distribution_method_dict = {\"linear\":self.linear_distribution,\n",
        "                                \"pareto\":self.paretto_distribution,\n",
        "                                \"fibonacci\":self.fibo_distribution}\n",
        "    return distribution_method_dict.get(self.distribution, self.invalid_distribution)\n",
        "    \n",
        "  def linear_distribution(self, i):\n",
        "    \"\"\"\n",
        "    This method defines the Flops to insert an early exits, according to a linear distribution.\n",
        "    \"\"\"\n",
        "    flop_margin = 1.0 / (self.n_branches+1)\n",
        "    return self.total_flops * flop_margin * (i+1)\n",
        "\n",
        "  def paretto_distribution(self, i):\n",
        "    \"\"\"\n",
        "    This method defines the Flops to insert an early exits, according to a pareto distribution.\n",
        "    \"\"\"\n",
        "    return self.total_flops * (1 - (0.8**(i+1)))\n",
        "\n",
        "  def fibo_distribution(self, i):\n",
        "    \"\"\"\n",
        "    This method defines the Flops to insert an early exits, according to a fibonacci distribution.\n",
        "    \"\"\"\n",
        "    gold_rate = 1.61803398875\n",
        "    return total_flops * (gold_rate**(i - self.num_ee))\n",
        "\n",
        "  def verifies_nr_exits(self, backbone_model):\n",
        "    \"\"\"\n",
        "    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_layers = len(list(backbone_model.children()))\n",
        "    if (self.n_branches >= total_layers):\n",
        "      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n",
        "\n",
        "  def countFlops(self, model):\n",
        "    \"\"\"\n",
        "    This method counts the numper of Flops in a given full DNN model or intermediate DNN model.\n",
        "    \"\"\"\n",
        "    input = torch.rand(1, self.channel, self.width, self.height)\n",
        "    flops, all_data = count_ops(model, input, print_readable=False, verbose=False)\n",
        "    return flops\n",
        "\n",
        "  def where_insert_early_exits(self):\n",
        "    \"\"\"\n",
        "    This method defines where insert the early exits, according to the dsitribution method selected.\n",
        "    Args:\n",
        "\n",
        "    total_flops: Flops of the backbone (full) DNN model.\n",
        "    \"\"\"\n",
        "    threshold_flop_list = []\n",
        "    distribution_method = self.select_distribution_method()\n",
        "\n",
        "    for i in range(self.n_branches):\n",
        "      threshold_flop_list.append(distribution_method(i))\n",
        "\n",
        "    return threshold_flop_list\n",
        "\n",
        "  def invalid_model(self):\n",
        "    raise Exception(\"This DNN model has not implemented yet.\")\n",
        "  def invalid_distribution(self):\n",
        "    raise Exception(\"This early-exit distribution has not implemented yet.\")\n",
        "\n",
        "  def is_suitable_for_exit(self):\n",
        "    \"\"\"\n",
        "    This method answers the following question. Is the position to place an early exit?\n",
        "    \"\"\"\n",
        "    intermediate_model = nn.Sequential(*(list(self.stages)+list(self.layers)))\n",
        "    current_flop = self.countFlops(intermediate_model)\n",
        "    return self.stage_id < self.n_branches and current_flop >= self.threshold_flop_list[self.stage_id]\n",
        "\n",
        "  def add_exit_block(self):\n",
        "    \"\"\"\n",
        "    This method adds an early exit in the suitable position.\n",
        "    \"\"\"\n",
        "    input_tensor = torch.rand(1, self.channel, self.width, self.height)\n",
        "\n",
        "    self.stages.append(nn.Sequential(*self.layers))\n",
        "\n",
        "    feature_shape = nn.Sequential(*self.stages)(input_tensor).shape\n",
        "\n",
        "    self.exits.append(EarlyExitBlock(feature_shape, self.n_classes, self.exit_type, self.device).to(self.device))\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.stage_id += 1    \n",
        "\n",
        "  def set_device(self):\n",
        "    \"\"\"\n",
        "    This method sets the device that will run the DNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    self.stages.to(self.device)\n",
        "    self.exits.to(self.device)\n",
        "    self.layers.to(self.device)\n",
        "    self.classifier.to(self.device)\n",
        "\n",
        "\n",
        "  def early_exit_alexnet(self):\n",
        "    \"\"\"\n",
        "    This method inserts early exits into a Alexnet model\n",
        "    \"\"\"\n",
        "\n",
        "    self.stages = nn.ModuleList()\n",
        "    self.exits = nn.ModuleList()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.cost = []\n",
        "    self.stage_id = 0\n",
        "\n",
        "    # Loads the backbone model. In other words, Alexnet architecture provided by Pytorch.\n",
        "    backbone_model = models.alexnet(self.pretrained)\n",
        "\n",
        "    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n",
        "    self.verifies_nr_exit_alexnet(backbone_model.features)\n",
        "    \n",
        "    # This obtains the flops total of the backbone model\n",
        "    self.total_flops = self.countFlops(backbone_model)\n",
        "\n",
        "    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n",
        "    self.threshold_flop_list = self.where_insert_early_exits()\n",
        "\n",
        "    for layer in backbone_model.features:\n",
        "      self.layers.append(layer)\n",
        "      if (isinstance(layer, nn.ReLU)) and (self.is_suitable_for_exit()):\n",
        "        self.add_exit_block()\n",
        "\n",
        "    \n",
        "    \n",
        "    self.layers.append(nn.AdaptiveAvgPool2d(output_size=(6, 6)))\n",
        "    self.stages.append(nn.Sequential(*self.layers))\n",
        "\n",
        "    \n",
        "    self.classifier = backbone_model.classifier\n",
        "    self.classifier[6] = nn.Linear(in_features=4096, out_features=self.n_classes, bias=True)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.set_device()\n",
        "\n",
        "  def verifies_nr_exit_alexnet(self, backbone_model):\n",
        "    \"\"\"\n",
        "    This method verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n",
        "    In AlexNet, we consider a convolutional block composed by: Convolutional layer, ReLU and he Max-pooling layer.\n",
        "    Hence, we consider that it makes no sense to insert side branches between these layers or only after the convolutional layer.\n",
        "    \"\"\"\n",
        "\n",
        "    count_relu_layer = 0\n",
        "    for layer in backbone_model:\n",
        "      if (isinstance(layer, nn.ReLU)):\n",
        "        count_relu_layer += 1\n",
        "\n",
        "    if (count_relu_layer < self.n_branches):\n",
        "      raise Exception(\"The number of early exits is greater than number of layers in the DNN backbone model.\")\n",
        "\n",
        "  def early_exit_resnet18(self):\n",
        "    \"\"\"\n",
        "    This method inserts early exits into a Resnet18 model\n",
        "    \"\"\"\n",
        "\n",
        "    self.stages = nn.ModuleList()\n",
        "    self.exits = nn.ModuleList()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.cost = []\n",
        "    self.stage_id = 0\n",
        "\n",
        "    self.inplanes = 64\n",
        "\n",
        "    n_blocks = 4\n",
        "\n",
        "    backbone_model = models.resnet18(self.pretrained)\n",
        "\n",
        "    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n",
        "    self.verifies_nr_exits(backbone_model)\n",
        "\n",
        "    # This obtains the flops total of the backbone model\n",
        "    self.total_flops = self.countFlops(backbone_model)\n",
        "\n",
        "    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n",
        "    self.threshold_flop_list = self.where_insert_early_exits()\n",
        "\n",
        "    building_first_layer = [\"conv1\", \"bn1\", \"relu\", \"maxpool\"]\n",
        "    for layer in building_first_layer:\n",
        "      self.layers.append(getattr(backbone_model, layer))\n",
        "\n",
        "    if (self.is_suitable_for_exit()):\n",
        "      self.add_exit_block()\n",
        "\n",
        "    for i in range(1, n_blocks+1):\n",
        "      \n",
        "      block_layer = getattr(backbone_model, \"layer%s\"%(i))\n",
        "\n",
        "      for l in block_layer:\n",
        "        self.layers.append(l)\n",
        "\n",
        "        if (self.is_suitable_for_exit()):\n",
        "          self.add_exit_block()\n",
        "    \n",
        "    self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
        "    self.classifier = nn.Sequential(nn.Linear(512, self.n_classes))\n",
        "    self.stages.append(nn.Sequential(*self.layers))\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.set_device()\n",
        "\n",
        "  def early_exit_resnet34(self):\n",
        "    return True\n",
        "  \n",
        "\n",
        "  def early_exit_mobilenet(self):\n",
        "    \"\"\"\n",
        "    This method inserts early exits into a Mobilenet V2 model\n",
        "    \"\"\"\n",
        "\n",
        "    self.stages = nn.ModuleList()\n",
        "    self.exits = nn.ModuleList()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.cost = []\n",
        "    self.stage_id = 0\n",
        "\n",
        "    last_channel = 1280\n",
        "    \n",
        "    # Loads the backbone model. In other words, Mobilenet architecture provided by Pytorch.\n",
        "    backbone_model = models.mobilenet_v2(self.pretrained)\n",
        "\n",
        "    # It verifies if the number of early exits provided is greater than a number of layers in the backbone DNN model.\n",
        "    self.verifies_nr_exits(backbone_model.features)\n",
        "    \n",
        "    # This obtains the flops total of the backbone model\n",
        "    self.total_flops = self.countFlops(backbone_model)\n",
        "\n",
        "    # This line obtains where inserting an early exit based on the Flops number and accordint to distribution method\n",
        "    self.threshold_flop_list = self.where_insert_early_exits()\n",
        "\n",
        "    for i, layer in enumerate(backbone_model.features.children()):\n",
        "      \n",
        "      self.layers.append(layer)    \n",
        "      if (self.is_suitable_for_exit()):\n",
        "        self.add_exit_block()\n",
        "\n",
        "    self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
        "    self.stages.append(nn.Sequential(*self.layers))\n",
        "    \n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(last_channel, self.n_classes),)\n",
        "\n",
        "    self.set_device()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forwardTrain(self, x):\n",
        "    \"\"\"\n",
        "    This method is used to train the early-exit DNN model\n",
        "    \"\"\"\n",
        "    \n",
        "    output_list, conf_list, class_list  = [], [], []\n",
        "\n",
        "    for i, exitBlock in enumerate(self.exits):\n",
        "      #print(i)\n",
        "      x = self.stages[i](x)\n",
        "      output_branch = exitBlock(x)\n",
        "      output_list.append(output_branch)\n",
        "\n",
        "      #Confidence is the maximum probability of belongs one of the predefined classes and inference_class is the argmax\n",
        "      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n",
        "      conf_list.append(conf)\n",
        "      class_list.append(infered_class)\n",
        "\n",
        "    x = self.stages[-1](x)\n",
        "\n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    output = self.classifier(x)\n",
        "    infered_conf, infered_class = torch.max(self.softmax(output), 1)\n",
        "    output_list.append(output)\n",
        "    conf_list.append(infered_conf)\n",
        "    class_list.append(infered_class)\n",
        "\n",
        "    return output_list, conf_list, class_list\n",
        "\n",
        "  def forwardEval(self, x, p_tar):\n",
        "    \"\"\"\n",
        "    This method is used to train the early-exit DNN model\n",
        "    \"\"\"\n",
        "    output_list, conf_list, class_list  = [], [], []\n",
        "\n",
        "    for i, exitBlock in enumerate(self.exits):\n",
        "      x = self.stages[i](x)\n",
        "\n",
        "      output_branch = exitBlock(x)\n",
        "      conf, infered_class = torch.max(self.softmax(output_branch), 1)\n",
        "\n",
        "      # Note that if confidence value is greater than a p_tar value, we terminate the dnn inference and returns the output\n",
        "      if (conf.item() >= p_tar):\n",
        "        return output_branch, conf, infered_class, i+1\n",
        "\n",
        "      else:\n",
        "        output_list.append(output_branch)\n",
        "        conf_list.append(conf)\n",
        "        class_list.append(infered_class)\n",
        "\n",
        "    x = self.stages[-1](x)\n",
        "    \n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    output = self.classifier(x)\n",
        "    conf, infered_class = torch.max(self.softmax(output), 1)\n",
        "    \n",
        "    # Note that if confidence value is greater than a p_tar value, we terminate the dnn inference and returns the output\n",
        "    # This also happens in the last exit\n",
        "    if (conf.item() >= p_tar):\n",
        "      return output, conf, infered_class, self.n_branches \n",
        "    else:\n",
        "\n",
        "      # If any exit can reach the p_tar value, the output is give by the more confidence output.\n",
        "      # If evaluation, it returns max(output), max(conf) and the number of the early exit.\n",
        "\n",
        "      conf_list.append(conf)\n",
        "      class_list.append(infered_class)\n",
        "      output_list.append(output)\n",
        "      max_conf = np.argmax(conf_list)\n",
        "      return output_list[max_conf], conf_list[max_conf], class_list[max_conf], self.n_branches\n",
        "\n",
        "  def forward(self, x, p_tar=0.5, training=True):\n",
        "    \"\"\"\n",
        "    This implementation supposes that, during training, this method can receive a batch containing multiple images.\n",
        "    However, during evaluation, this method supposes an only image.\n",
        "    \"\"\"\n",
        "    if (training):\n",
        "      return self.forwardTrain(x)\n",
        "    else:\n",
        "      return self.forwardEval(x, p_tar)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLybDY8v7Hu8"
      },
      "source": [
        "def trainBranches(model, train_loader, optimizer, criterion, n_branches, epoch, device, loss_weights):\n",
        "  running_loss = []\n",
        "  n_exits = n_branches + 1\n",
        "  train_acc_dict = {i: [] for i in range(1, (n_exits)+1)}\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for i, (data, target) in enumerate(train_loader, 1):\n",
        "    #print(\"Batch: %s/%s\"%(i, len(train_loader)))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    output_list, conf_list, class_list = model(data)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for j, (output, inf_class, weight) in enumerate(zip(output_list, class_list, loss_weights), 1):\n",
        "      loss += weight*criterion(output, target)\n",
        "      train_acc_dict[j].append(100*inf_class.eq(target.view_as(inf_class)).sum().item()/target.size(0))\n",
        "\n",
        "    running_loss.append(float(loss.item()))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "\n",
        "    # clear variables\n",
        "    del data, target, output_list, conf_list, class_list\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  loss = round(np.average(running_loss), 4)\n",
        "  print(\"Epoch: %s\"%(epoch))\n",
        "  print(\"Train Loss: %s\"%(loss))\n",
        "\n",
        "  result_dict = {\"epoch\":epoch, \"train_loss\": loss}\n",
        "  for key, value in train_acc_dict.items():\n",
        "    result_dict.update({\"train_acc_branch_%s\"%(key): round(np.average(train_acc_dict[key]), 4)})    \n",
        "    print(\"Train Acc Branch %s: %s\"%(key, result_dict[\"train_acc_branch_%s\"%(key)]))\n",
        "  \n",
        "  return result_dict\n",
        "\n",
        "def evalBranches(model, val_loader, criterion, n_branches, epoch, device):\n",
        "  running_loss = []\n",
        "  val_acc_dict = {i: [] for i in range(1, (n_branches+1)+1)}\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (data, target) in enumerate(val_loader, 1):\n",
        "      #print(\"Batch: %s / %s\"%(i, len(val_loader)))\n",
        "      data, target = data.to(device), target.long().to(device)\n",
        "\n",
        "      output_list, conf_list, class_list = model(data)\n",
        "      loss = 0\n",
        "      for j, (output, inf_class, weight) in enumerate(zip(output_list, class_list, loss_weights), 1):\n",
        "        loss += weight*criterion(output, target)\n",
        "        val_acc_dict[j].append(100*inf_class.eq(target.view_as(inf_class)).sum().item()/target.size(0))\n",
        "\n",
        "\n",
        "      running_loss.append(float(loss.item()))    \n",
        "\n",
        "      # clear variables\n",
        "      del data, target, output_list, conf_list, class_list\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  loss = round(np.average(running_loss), 4)\n",
        "  print(\"Epoch: %s\"%(epoch))\n",
        "  print(\"Val Loss: %s\"%(loss))\n",
        "\n",
        "  result_dict = {\"epoch\":epoch, \"val_loss\": loss}\n",
        "  for key, value in val_acc_dict.items():\n",
        "    result_dict.update({\"val_acc_branch_%s\"%(key): round(np.average(val_acc_dict[key]), 4)})    \n",
        "    print(\"Val Acc Branch %s: %s\"%(key, result_dict[\"val_acc_branch_%s\"%(key)]))\n",
        "  \n",
        "  return result_dict\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eBQEYRlEfP8",
        "outputId": "9afc0af0-da65-4934-9074-872060ec53fe"
      },
      "source": [
        "model_name = \"alexnet\"\n",
        "dataset_name = \"cifar_10\"\n",
        "model_id = 1\n",
        "img_dim = 300\n",
        "input_dim = 300\n",
        "batch_size_train, batch_size_test = 32, 1\n",
        "split_ratio = 0.1\n",
        "save_idx = False\n",
        "\n",
        "\n",
        "root_dir = \"./early_exit_test\"\n",
        "dataset_path = \"./\"\n",
        "\n",
        "save_root_path = os.path.join(root_dir, dataset_name, model_name)\n",
        "if (not os.path.exists(save_root_path)):\n",
        "  os.makedirs(save_root_path)\n",
        "\n",
        "\n",
        "model_save_path = os.path.join(save_root_path, \"%s_%s_%s.pth\"%(model_name, dataset_name, model_id))\n",
        "history_save_path = os.path.join(save_root_path, \"history_%s_%s_%s.pth\"%(model_name, dataset_name, model_id))\n",
        "\n",
        "dataset = LoadDataset(img_dim, batch_size_train, batch_size_test, save_idx, model_id)\n",
        "train_loader, val_loader, test_loader = dataset.cifar_10(dataset_path, split_ratio)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VbcCIxUR89-",
        "outputId": "506edc87-c075-40b4-992c-9c2820b218da"
      },
      "source": [
        "model_id = 1\n",
        "n_classes = 10\n",
        "pretrained = True\n",
        "n_branches = 2\n",
        "n_exits = n_branches + 1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_shape = (3, input_dim, input_dim)\n",
        "distribution = \"linear\"\n",
        "exit_type = \"bnpool\"\n",
        "lr = [1.5e-4, 0.005]\n",
        "weight_decay = 0.00005\n",
        "\n",
        "\n",
        "\n",
        "early_exit_model = Early_Exit_DNN(model_name, n_classes, pretrained, n_branches, input_shape, exit_type, device, distribution=distribution)\n",
        "early_exit_model = early_exit_model.to(device)\n",
        "early_exit_model.exits.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam([{'params': early_exit_model.stages.parameters(), 'lr': lr[0]},\n",
        "                       {'params': early_exit_model.exits.parameters(), 'lr': lr[1]},\n",
        "                       {'params': early_exit_model.classifier.parameters(), 'lr': lr[1]}], weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0, last_epoch=-1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size: (1, 3, 300, 300)\n",
            "1,206,314,600 FLOPs or approx. 1.21 GFLOPs\n",
            "Input size: (1, 3, 300, 300)\n",
            "128,269,824 FLOPs or approx. 0.13 GFLOPs\n",
            "Input size: (1, 3, 300, 300)\n",
            "527,811,072 FLOPs or approx. 0.53 GFLOPs\n",
            "Input size: (1, 3, 300, 300)\n",
            "720,354,432 FLOPs or approx. 0.72 GFLOPs\n",
            "Input size: (1, 3, 300, 300)\n",
            "976,265,088 FLOPs or approx. 0.98 GFLOPs\n",
            "Input size: (1, 3, 300, 300)\n",
            "1,146,946,176 FLOPs or approx. 1.15 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0UGQsDKxYuk",
        "outputId": "b26c805f-fbc1-40a9-cda6-8be4d3474d23"
      },
      "source": [
        "loss_weights = np.linspace(0.15, 1, n_exits)\n",
        "\n",
        "epoch = 0\n",
        "count = 0\n",
        "best_val_loss = np.inf\n",
        "patience = 10\n",
        "\n",
        "df = pd.DataFrame()\n",
        "while 1:\n",
        "  epoch+=1\n",
        "  print(\"Epoch: %s\"%(epoch))\n",
        "  result = {}\n",
        "\n",
        "  result.update(trainBranches(early_exit_model, train_loader, optimizer, criterion, n_branches, epoch, device, loss_weights))\n",
        "  print(result)\n",
        "  scheduler.step()\n",
        "  result.update(evalBranches(early_exit_model, val_loader, criterion, n_branches, epoch, device))\n",
        "\n",
        "  df = df.append(pd.Series(result), ignore_index=True)\n",
        "  df.to_csv(history_save_path)\n",
        "\n",
        "  if (result[\"val_loss\"] < best_val_loss):\n",
        "    best_val_loss = result[\"val_loss\"]\n",
        "    count = 0\n",
        "    save_dict = {\"model_state_dict\": early_exit_model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                 \"epoch\": epoch, \"val_loss\": result[\"val_loss\"]}\n",
        "    \n",
        "    for i in range(1, n_branches+1+1):\n",
        "      save_dict.update({\"val_acc_branch_%s\"%(i): result[\"val_acc_branch_%s\"%(i)]})\n",
        "\n",
        "    torch.save(save_dict, model_save_path)\n",
        "\n",
        "  else:\n",
        "    count += 1\n",
        "    if (count > patience):\n",
        "      print(\"Stop! Patience is finished\")\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 2.7288\n",
            "Train Acc Branch 1: 45.036\n",
            "Train Acc Branch 2: 60.2545\n",
            "Train Acc Branch 3: 35.159\n",
            "{'epoch': 1, 'train_loss': 2.7288, 'train_acc_branch_1': 45.036, 'train_acc_branch_2': 60.2545, 'train_acc_branch_3': 35.159}\n",
            "Epoch: 1\n",
            "Val Loss: 2.1394\n",
            "Val Acc Branch 1: 50.1\n",
            "Val Acc Branch 2: 67.88\n",
            "Val Acc Branch 3: 48.1\n",
            "Epoch: 2\n",
            "Epoch: 2\n",
            "Train Loss: 2.003\n",
            "Train Acc Branch 1: 51.1771\n",
            "Train Acc Branch 2: 71.8284\n",
            "Train Acc Branch 3: 53.9845\n",
            "{'epoch': 2, 'train_loss': 2.003, 'train_acc_branch_1': 51.1771, 'train_acc_branch_2': 71.8284, 'train_acc_branch_3': 53.9845}\n",
            "Epoch: 2\n",
            "Val Loss: 1.7765\n",
            "Val Acc Branch 1: 51.24\n",
            "Val Acc Branch 2: 73.22\n",
            "Val Acc Branch 3: 60.36\n",
            "Epoch: 3\n",
            "Epoch: 3\n",
            "Train Loss: 1.6675\n",
            "Train Acc Branch 1: 53.4959\n",
            "Train Acc Branch 2: 75.9728\n",
            "Train Acc Branch 3: 63.977\n",
            "{'epoch': 3, 'train_loss': 1.6675, 'train_acc_branch_1': 53.4959, 'train_acc_branch_2': 75.9728, 'train_acc_branch_3': 63.977}\n",
            "Epoch: 3\n",
            "Val Loss: 1.4609\n",
            "Val Acc Branch 1: 54.68\n",
            "Val Acc Branch 2: 76.58\n",
            "Val Acc Branch 3: 70.12\n",
            "Epoch: 4\n",
            "Epoch: 4\n",
            "Train Loss: 1.4533\n",
            "Train Acc Branch 1: 55.2372\n",
            "Train Acc Branch 2: 78.6469\n",
            "Train Acc Branch 3: 70.4935\n",
            "{'epoch': 4, 'train_loss': 1.4533, 'train_acc_branch_1': 55.2372, 'train_acc_branch_2': 78.6469, 'train_acc_branch_3': 70.4935}\n",
            "Epoch: 4\n",
            "Val Loss: 1.3264\n",
            "Val Acc Branch 1: 55.78\n",
            "Val Acc Branch 2: 78.34\n",
            "Val Acc Branch 3: 73.52\n",
            "Epoch: 5\n",
            "Epoch: 5\n",
            "Train Loss: 1.2658\n",
            "Train Acc Branch 1: 56.6587\n",
            "Train Acc Branch 2: 80.9746\n",
            "Train Acc Branch 3: 75.3887\n",
            "{'epoch': 5, 'train_loss': 1.2658, 'train_acc_branch_1': 56.6587, 'train_acc_branch_2': 80.9746, 'train_acc_branch_3': 75.3887}\n",
            "Epoch: 5\n",
            "Val Loss: 1.1416\n",
            "Val Acc Branch 1: 57.72\n",
            "Val Acc Branch 2: 81.2\n",
            "Val Acc Branch 3: 78.32\n",
            "Epoch: 6\n",
            "Epoch: 6\n",
            "Train Loss: 1.1153\n",
            "Train Acc Branch 1: 58.1979\n",
            "Train Acc Branch 2: 82.8181\n",
            "Train Acc Branch 3: 78.8868\n",
            "{'epoch': 6, 'train_loss': 1.1153, 'train_acc_branch_1': 58.1979, 'train_acc_branch_2': 82.8181, 'train_acc_branch_3': 78.8868}\n",
            "Epoch: 6\n",
            "Val Loss: 1.0624\n",
            "Val Acc Branch 1: 59.68\n",
            "Val Acc Branch 2: 82.02\n",
            "Val Acc Branch 3: 80.06\n",
            "Epoch: 7\n",
            "Epoch: 7\n",
            "Train Loss: 0.989\n",
            "Train Acc Branch 1: 59.2995\n",
            "Train Acc Branch 2: 84.4439\n",
            "Train Acc Branch 3: 81.9674\n",
            "{'epoch': 7, 'train_loss': 0.989, 'train_acc_branch_1': 59.2995, 'train_acc_branch_2': 84.4439, 'train_acc_branch_3': 81.9674}\n",
            "Epoch: 7\n",
            "Val Loss: 0.9693\n",
            "Val Acc Branch 1: 60.88\n",
            "Val Acc Branch 2: 83.2\n",
            "Val Acc Branch 3: 82.9\n",
            "Epoch: 8\n",
            "Epoch: 8\n",
            "Train Loss: 0.8846\n",
            "Train Acc Branch 1: 60.1257\n",
            "Train Acc Branch 2: 85.7543\n",
            "Train Acc Branch 3: 84.2462\n",
            "{'epoch': 8, 'train_loss': 0.8846, 'train_acc_branch_1': 60.1257, 'train_acc_branch_2': 85.7543, 'train_acc_branch_3': 84.2462}\n",
            "Epoch: 8\n",
            "Val Loss: 0.8778\n",
            "Val Acc Branch 1: 61.5\n",
            "Val Acc Branch 2: 84.84\n",
            "Val Acc Branch 3: 84.68\n",
            "Epoch: 9\n",
            "Epoch: 9\n",
            "Train Loss: 0.8079\n",
            "Train Acc Branch 1: 60.8653\n",
            "Train Acc Branch 2: 86.576\n",
            "Train Acc Branch 3: 86.3717\n",
            "{'epoch': 9, 'train_loss': 0.8079, 'train_acc_branch_1': 60.8653, 'train_acc_branch_2': 86.576, 'train_acc_branch_3': 86.3717}\n",
            "Epoch: 9\n",
            "Val Loss: 0.8528\n",
            "Val Acc Branch 1: 62.0\n",
            "Val Acc Branch 2: 85.32\n",
            "Val Acc Branch 3: 85.22\n",
            "Epoch: 10\n",
            "Epoch: 10\n",
            "Train Loss: 0.7588\n",
            "Train Acc Branch 1: 61.334\n",
            "Train Acc Branch 2: 86.9758\n",
            "Train Acc Branch 3: 87.6821\n",
            "{'epoch': 10, 'train_loss': 0.7588, 'train_acc_branch_1': 61.334, 'train_acc_branch_2': 86.9758, 'train_acc_branch_3': 87.6821}\n",
            "Epoch: 10\n",
            "Val Loss: 0.8352\n",
            "Val Acc Branch 1: 61.54\n",
            "Val Acc Branch 2: 84.86\n",
            "Val Acc Branch 3: 86.1\n",
            "Epoch: 11\n",
            "Epoch: 11\n",
            "Train Loss: 0.7453\n",
            "Train Acc Branch 1: 60.9986\n",
            "Train Acc Branch 2: 87.209\n",
            "Train Acc Branch 3: 87.8865\n",
            "{'epoch': 11, 'train_loss': 0.7453, 'train_acc_branch_1': 60.9986, 'train_acc_branch_2': 87.209, 'train_acc_branch_3': 87.8865}\n",
            "Epoch: 11\n",
            "Val Loss: 0.8268\n",
            "Val Acc Branch 1: 61.98\n",
            "Val Acc Branch 2: 84.82\n",
            "Val Acc Branch 3: 86.18\n",
            "Epoch: 12\n",
            "Epoch: 12\n",
            "Train Loss: 0.7481\n",
            "Train Acc Branch 1: 61.2429\n",
            "Train Acc Branch 2: 87.1224\n",
            "Train Acc Branch 3: 87.9709\n",
            "{'epoch': 12, 'train_loss': 0.7481, 'train_acc_branch_1': 61.2429, 'train_acc_branch_2': 87.1224, 'train_acc_branch_3': 87.9709}\n",
            "Epoch: 12\n",
            "Val Loss: 0.8153\n",
            "Val Acc Branch 1: 62.38\n",
            "Val Acc Branch 2: 85.12\n",
            "Val Acc Branch 3: 86.1\n",
            "Epoch: 13\n",
            "Epoch: 13\n",
            "Train Loss: 0.7713\n",
            "Train Acc Branch 1: 61.1874\n",
            "Train Acc Branch 2: 87.0713\n",
            "Train Acc Branch 3: 87.0913\n",
            "{'epoch': 13, 'train_loss': 0.7713, 'train_acc_branch_1': 61.1874, 'train_acc_branch_2': 87.0713, 'train_acc_branch_3': 87.0913}\n",
            "Epoch: 13\n",
            "Val Loss: 0.823\n",
            "Val Acc Branch 1: 62.28\n",
            "Val Acc Branch 2: 85.24\n",
            "Val Acc Branch 3: 85.94\n",
            "Epoch: 14\n",
            "Epoch: 14\n",
            "Train Loss: 0.8077\n",
            "Train Acc Branch 1: 60.6787\n",
            "Train Acc Branch 2: 86.8826\n",
            "Train Acc Branch 3: 86.2251\n",
            "{'epoch': 14, 'train_loss': 0.8077, 'train_acc_branch_1': 60.6787, 'train_acc_branch_2': 86.8826, 'train_acc_branch_3': 86.2251}\n",
            "Epoch: 14\n",
            "Val Loss: 0.8839\n",
            "Val Acc Branch 1: 61.26\n",
            "Val Acc Branch 2: 84.94\n",
            "Val Acc Branch 3: 84.66\n",
            "Epoch: 15\n",
            "Epoch: 15\n",
            "Train Loss: 0.8583\n",
            "Train Acc Branch 1: 60.259\n",
            "Train Acc Branch 2: 86.5894\n",
            "Train Acc Branch 3: 84.7703\n",
            "{'epoch': 15, 'train_loss': 0.8583, 'train_acc_branch_1': 60.259, 'train_acc_branch_2': 86.5894, 'train_acc_branch_3': 84.7703}\n",
            "Epoch: 15\n",
            "Val Loss: 0.8619\n",
            "Val Acc Branch 1: 61.74\n",
            "Val Acc Branch 2: 85.92\n",
            "Val Acc Branch 3: 85.22\n",
            "Epoch: 16\n",
            "Epoch: 16\n",
            "Train Loss: 0.9116\n",
            "Train Acc Branch 1: 60.0635\n",
            "Train Acc Branch 2: 86.0697\n",
            "Train Acc Branch 3: 83.6065\n",
            "{'epoch': 16, 'train_loss': 0.9116, 'train_acc_branch_1': 60.0635, 'train_acc_branch_2': 86.0697, 'train_acc_branch_3': 83.6065}\n",
            "Epoch: 16\n",
            "Val Loss: 0.9688\n",
            "Val Acc Branch 1: 59.5\n",
            "Val Acc Branch 2: 84.04\n",
            "Val Acc Branch 3: 82.82\n",
            "Epoch: 17\n",
            "Epoch: 17\n",
            "Train Loss: 0.978\n",
            "Train Acc Branch 1: 59.4416\n",
            "Train Acc Branch 2: 85.581\n",
            "Train Acc Branch 3: 82.1429\n",
            "{'epoch': 17, 'train_loss': 0.978, 'train_acc_branch_1': 59.4416, 'train_acc_branch_2': 85.581, 'train_acc_branch_3': 82.1429}\n",
            "Epoch: 17\n",
            "Val Loss: 0.9655\n",
            "Val Acc Branch 1: 60.5\n",
            "Val Acc Branch 2: 83.92\n",
            "Val Acc Branch 3: 82.42\n",
            "Epoch: 18\n",
            "Epoch: 18\n",
            "Train Loss: 1.0252\n",
            "Train Acc Branch 1: 59.6526\n",
            "Train Acc Branch 2: 85.1257\n",
            "Train Acc Branch 3: 80.9146\n",
            "{'epoch': 18, 'train_loss': 1.0252, 'train_acc_branch_1': 59.6526, 'train_acc_branch_2': 85.1257, 'train_acc_branch_3': 80.9146}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWbGgEy7D5qx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}